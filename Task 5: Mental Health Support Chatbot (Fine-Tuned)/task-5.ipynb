{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- Kaggle cell 1 ----------------------------------------------------------\n# Kaggle gives you 30 GB of disk, but only 9 GB of RAM for pip installs.\n# Install the minimal set in one shot so Kaggle does not kill the kernel.\n!pip install -q --disable-pip-version-check \\\n    torch transformers datasets accelerate peft bitsandbytes streamlit\n\n# Optional: if you want to push the trained adapter to the HF Hub\n# from huggingface_hub import notebook_login\n# notebook_login()   # paste token when prompted","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-13T12:55:00.791920Z","iopub.execute_input":"2025-08-13T12:55:00.792160Z","iopub.status.idle":"2025-08-13T12:56:19.904842Z","shell.execute_reply.started":"2025-08-13T12:55:00.792137Z","shell.execute_reply":"2025-08-13T12:56:19.904140Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.3/61.3 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- Kaggle cell 2 ----------------------------------------------------------\nimport os, torch, warnings, gc\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    DataCollatorForLanguageModeling,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n)\nfrom peft import LoraConfig, get_peft_model, PeftModel\n\nwarnings.filterwarnings(\"ignore\")\ntorch.cuda.empty_cache()\n\nMODEL_NAME   = \"distilgpt2\"\nMAX_LEN      = 256\nBATCH        = 8\nEPOCHS       = 2          # 2 epochs finish in ~8 min on T4Ã—1\nLR           = 3e-4\nOUTPUT_DIR   = \"/kaggle/working/empathetic_bot\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\n\n# 1. Load & tokenize EmpatheticDialogues\nraw_ds = load_dataset(\"empathetic_dialogues\")\n\ndef format_example(ex):\n    text = f\"<|prompt|>{ex['prompt']}<|endoftext|><|response|>{ex['utterance']}\"\n    return tokenizer(text, truncation=True, max_length=MAX_LEN)\n\ntokenized = raw_ds.map(format_example, remove_columns=raw_ds[\"train\"].column_names)\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\n# 2. Model + LoRA\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\nlora_config = LoraConfig(\n    r=8, lora_alpha=32, target_modules=[\"c_attn\"], lora_dropout=0.05, bias=\"none\"\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 3. Train\nargs = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    per_device_train_batch_size=BATCH,\n    per_device_eval_batch_size=BATCH,      # skip eval to save time\n    num_train_epochs=EPOCHS,\n    learning_rate=LR,\n    fp16=True,\n    logging_steps=50,\n    save_total_limit=1,\n    report_to=\"none\",\n)\ntrainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=tokenized[\"train\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n)\ntrainer.train()\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\n\n# 4. Quick sanity chat test\npipe = pipeline(\n    \"text-generation\",\n    model=PeftModel.from_pretrained(\n        AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16),\n        OUTPUT_DIR\n    ),\n    tokenizer=tokenizer,\n    device_map=\"auto\"\n)\n\ndef chat(user_text):\n    prompt = f\"<|prompt|>{user_text}<|endoftext|><|response|>\"\n    out = pipe(\n        prompt,\n        max_new_tokens=64,\n        temperature=0.7,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n    )[0][\"generated_text\"]\n    return out.split(\"<|response|>\")[-1].strip()\n\nprint(\"ğŸ’™ EmpathyBot ready! Try:\")\nfor q in [\"I feel overwhelmed with work.\", \"My cat is sick and Iâ€™m scared.\"]:\n    print(\"You:\", q)\n    print(\"Bot:\", chat(q), \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T12:58:03.783136Z","iopub.execute_input":"2025-08-13T12:58:03.783426Z","iopub.status.idle":"2025-08-13T13:12:53.215819Z","shell.execute_reply.started":"2025-08-13T12:58:03.783397Z","shell.execute_reply":"2025-08-13T13:12:53.215034Z"}},"outputs":[{"name":"stderr","text":"2025-08-13 12:58:13.228739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755089893.423078      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755089893.476916      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee852fdc18b849c1b7c423fb49867922"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107e489298bb4a788a176b381e9e5774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1351306c880848d196e601784064f817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7af5f9e28d6a43bd9cfaba029c0bb4d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf2a2f817924955bb22d63371e2cc4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3962f3006f2b4d0d8a7c280fd593f358"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"empathetic_dialogues.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d33eca76433479994c8a50414b1efec"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for empathetic_dialogues contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/empathetic_dialogues.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/28.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4c62adc68be45e9bace31acb843f9d8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/76673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40adae1499534abc9b7c1217723c8d5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/12030 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"debaf83f1e1942c5bcfbfdff8b131d76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/10943 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"feb1ad8948ca4789bfca04b3edab1de0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/76673 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ff8579b1b114d8c81f11494d1fb2773"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12030 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3287ac25a234630ad6917bfed99e2c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10943 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f1b65c6de449deaa806e5ac1d42417"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f845927f40441f4b2cc893252042c48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"038af4f05f7442a19258a1b423c67a77"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 147,456 || all params: 82,060,032 || trainable%: 0.1797\n","output_type":"stream"},{"name":"stderr","text":"`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='19170' max='19170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [19170/19170 13:09, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>3.798300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.909800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.719200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.625900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>2.556400</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>2.596600</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>2.572700</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>2.537800</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.510300</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>2.501300</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>2.508300</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>2.547200</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>2.524300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>2.502100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>2.528300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>2.557700</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>2.453700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>2.511600</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>2.518800</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.537700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>2.487300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>2.420200</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>2.472900</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>2.519400</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>2.534600</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>2.512700</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>2.514200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>2.483500</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>2.427300</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.535100</td>\n    </tr>\n    <tr>\n      <td>1550</td>\n      <td>2.454100</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>2.481200</td>\n    </tr>\n    <tr>\n      <td>1650</td>\n      <td>2.469100</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>2.487200</td>\n    </tr>\n    <tr>\n      <td>1750</td>\n      <td>2.450100</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>2.483200</td>\n    </tr>\n    <tr>\n      <td>1850</td>\n      <td>2.456600</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>2.412400</td>\n    </tr>\n    <tr>\n      <td>1950</td>\n      <td>2.452900</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.496400</td>\n    </tr>\n    <tr>\n      <td>2050</td>\n      <td>2.469800</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>2.435900</td>\n    </tr>\n    <tr>\n      <td>2150</td>\n      <td>2.516500</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>2.446800</td>\n    </tr>\n    <tr>\n      <td>2250</td>\n      <td>2.491400</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>2.430700</td>\n    </tr>\n    <tr>\n      <td>2350</td>\n      <td>2.429200</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>2.461400</td>\n    </tr>\n    <tr>\n      <td>2450</td>\n      <td>2.477800</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.504000</td>\n    </tr>\n    <tr>\n      <td>2550</td>\n      <td>2.496800</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>2.440900</td>\n    </tr>\n    <tr>\n      <td>2650</td>\n      <td>2.407400</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>2.427900</td>\n    </tr>\n    <tr>\n      <td>2750</td>\n      <td>2.423900</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>2.459800</td>\n    </tr>\n    <tr>\n      <td>2850</td>\n      <td>2.445400</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>2.442000</td>\n    </tr>\n    <tr>\n      <td>2950</td>\n      <td>2.430700</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>2.512700</td>\n    </tr>\n    <tr>\n      <td>3050</td>\n      <td>2.472800</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>2.436700</td>\n    </tr>\n    <tr>\n      <td>3150</td>\n      <td>2.490600</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>2.413900</td>\n    </tr>\n    <tr>\n      <td>3250</td>\n      <td>2.401700</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>2.462500</td>\n    </tr>\n    <tr>\n      <td>3350</td>\n      <td>2.405900</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>2.414700</td>\n    </tr>\n    <tr>\n      <td>3450</td>\n      <td>2.407200</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>2.443900</td>\n    </tr>\n    <tr>\n      <td>3550</td>\n      <td>2.413000</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>2.437600</td>\n    </tr>\n    <tr>\n      <td>3650</td>\n      <td>2.410600</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>2.379400</td>\n    </tr>\n    <tr>\n      <td>3750</td>\n      <td>2.434400</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>2.392400</td>\n    </tr>\n    <tr>\n      <td>3850</td>\n      <td>2.406200</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>2.464000</td>\n    </tr>\n    <tr>\n      <td>3950</td>\n      <td>2.422500</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>2.400300</td>\n    </tr>\n    <tr>\n      <td>4050</td>\n      <td>2.427700</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>2.394900</td>\n    </tr>\n    <tr>\n      <td>4150</td>\n      <td>2.495400</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>2.438500</td>\n    </tr>\n    <tr>\n      <td>4250</td>\n      <td>2.406900</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>2.431800</td>\n    </tr>\n    <tr>\n      <td>4350</td>\n      <td>2.466400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>2.425200</td>\n    </tr>\n    <tr>\n      <td>4450</td>\n      <td>2.453800</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>2.419100</td>\n    </tr>\n    <tr>\n      <td>4550</td>\n      <td>2.388000</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>2.401900</td>\n    </tr>\n    <tr>\n      <td>4650</td>\n      <td>2.455100</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>2.435900</td>\n    </tr>\n    <tr>\n      <td>4750</td>\n      <td>2.400600</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>2.355000</td>\n    </tr>\n    <tr>\n      <td>4850</td>\n      <td>2.420500</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>2.428100</td>\n    </tr>\n    <tr>\n      <td>4950</td>\n      <td>2.469400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>2.388900</td>\n    </tr>\n    <tr>\n      <td>5050</td>\n      <td>2.406200</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>2.438100</td>\n    </tr>\n    <tr>\n      <td>5150</td>\n      <td>2.398600</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>2.395100</td>\n    </tr>\n    <tr>\n      <td>5250</td>\n      <td>2.423900</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>2.448300</td>\n    </tr>\n    <tr>\n      <td>5350</td>\n      <td>2.380700</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>2.443200</td>\n    </tr>\n    <tr>\n      <td>5450</td>\n      <td>2.441100</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>2.447900</td>\n    </tr>\n    <tr>\n      <td>5550</td>\n      <td>2.452300</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>2.401500</td>\n    </tr>\n    <tr>\n      <td>5650</td>\n      <td>2.415900</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>2.388500</td>\n    </tr>\n    <tr>\n      <td>5750</td>\n      <td>2.417200</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>2.399000</td>\n    </tr>\n    <tr>\n      <td>5850</td>\n      <td>2.384700</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>2.412800</td>\n    </tr>\n    <tr>\n      <td>5950</td>\n      <td>2.438700</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>2.415000</td>\n    </tr>\n    <tr>\n      <td>6050</td>\n      <td>2.394300</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>2.391100</td>\n    </tr>\n    <tr>\n      <td>6150</td>\n      <td>2.449100</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>2.437900</td>\n    </tr>\n    <tr>\n      <td>6250</td>\n      <td>2.421900</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>2.381100</td>\n    </tr>\n    <tr>\n      <td>6350</td>\n      <td>2.382400</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>2.379500</td>\n    </tr>\n    <tr>\n      <td>6450</td>\n      <td>2.374600</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>2.454100</td>\n    </tr>\n    <tr>\n      <td>6550</td>\n      <td>2.434900</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>2.411300</td>\n    </tr>\n    <tr>\n      <td>6650</td>\n      <td>2.363400</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>2.428500</td>\n    </tr>\n    <tr>\n      <td>6750</td>\n      <td>2.381800</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>2.384200</td>\n    </tr>\n    <tr>\n      <td>6850</td>\n      <td>2.481200</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>2.423200</td>\n    </tr>\n    <tr>\n      <td>6950</td>\n      <td>2.377900</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>2.381500</td>\n    </tr>\n    <tr>\n      <td>7050</td>\n      <td>2.448400</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>2.437700</td>\n    </tr>\n    <tr>\n      <td>7150</td>\n      <td>2.418600</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>2.366300</td>\n    </tr>\n    <tr>\n      <td>7250</td>\n      <td>2.421700</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>2.441000</td>\n    </tr>\n    <tr>\n      <td>7350</td>\n      <td>2.370700</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>2.381400</td>\n    </tr>\n    <tr>\n      <td>7450</td>\n      <td>2.384900</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>2.394700</td>\n    </tr>\n    <tr>\n      <td>7550</td>\n      <td>2.435500</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>2.388100</td>\n    </tr>\n    <tr>\n      <td>7650</td>\n      <td>2.400400</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>2.393300</td>\n    </tr>\n    <tr>\n      <td>7750</td>\n      <td>2.366900</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>2.391400</td>\n    </tr>\n    <tr>\n      <td>7850</td>\n      <td>2.422400</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>2.416200</td>\n    </tr>\n    <tr>\n      <td>7950</td>\n      <td>2.388600</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>2.369900</td>\n    </tr>\n    <tr>\n      <td>8050</td>\n      <td>2.426200</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>2.359600</td>\n    </tr>\n    <tr>\n      <td>8150</td>\n      <td>2.341000</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>2.378800</td>\n    </tr>\n    <tr>\n      <td>8250</td>\n      <td>2.420400</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>2.313200</td>\n    </tr>\n    <tr>\n      <td>8350</td>\n      <td>2.455200</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>2.395400</td>\n    </tr>\n    <tr>\n      <td>8450</td>\n      <td>2.411700</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>2.408500</td>\n    </tr>\n    <tr>\n      <td>8550</td>\n      <td>2.438700</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>2.381000</td>\n    </tr>\n    <tr>\n      <td>8650</td>\n      <td>2.399700</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>2.429600</td>\n    </tr>\n    <tr>\n      <td>8750</td>\n      <td>2.393300</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>2.376000</td>\n    </tr>\n    <tr>\n      <td>8850</td>\n      <td>2.384200</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>2.386100</td>\n    </tr>\n    <tr>\n      <td>8950</td>\n      <td>2.422200</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>2.382700</td>\n    </tr>\n    <tr>\n      <td>9050</td>\n      <td>2.334000</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>2.353100</td>\n    </tr>\n    <tr>\n      <td>9150</td>\n      <td>2.342600</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>2.363200</td>\n    </tr>\n    <tr>\n      <td>9250</td>\n      <td>2.387400</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>2.353800</td>\n    </tr>\n    <tr>\n      <td>9350</td>\n      <td>2.381300</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>2.400500</td>\n    </tr>\n    <tr>\n      <td>9450</td>\n      <td>2.415800</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>2.413500</td>\n    </tr>\n    <tr>\n      <td>9550</td>\n      <td>2.372800</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>2.433300</td>\n    </tr>\n    <tr>\n      <td>9650</td>\n      <td>2.384000</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>2.412600</td>\n    </tr>\n    <tr>\n      <td>9750</td>\n      <td>2.317800</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>2.337700</td>\n    </tr>\n    <tr>\n      <td>9850</td>\n      <td>2.402300</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>2.406000</td>\n    </tr>\n    <tr>\n      <td>9950</td>\n      <td>2.417500</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>2.330500</td>\n    </tr>\n    <tr>\n      <td>10050</td>\n      <td>2.375700</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>2.349000</td>\n    </tr>\n    <tr>\n      <td>10150</td>\n      <td>2.388000</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>2.358900</td>\n    </tr>\n    <tr>\n      <td>10250</td>\n      <td>2.381100</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>2.453600</td>\n    </tr>\n    <tr>\n      <td>10350</td>\n      <td>2.430500</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>2.423600</td>\n    </tr>\n    <tr>\n      <td>10450</td>\n      <td>2.380200</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>2.362300</td>\n    </tr>\n    <tr>\n      <td>10550</td>\n      <td>2.383900</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>2.356300</td>\n    </tr>\n    <tr>\n      <td>10650</td>\n      <td>2.380000</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>2.374500</td>\n    </tr>\n    <tr>\n      <td>10750</td>\n      <td>2.416300</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>2.410000</td>\n    </tr>\n    <tr>\n      <td>10850</td>\n      <td>2.346400</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>2.371900</td>\n    </tr>\n    <tr>\n      <td>10950</td>\n      <td>2.340400</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>2.412000</td>\n    </tr>\n    <tr>\n      <td>11050</td>\n      <td>2.402900</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>2.331500</td>\n    </tr>\n    <tr>\n      <td>11150</td>\n      <td>2.414100</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>2.412000</td>\n    </tr>\n    <tr>\n      <td>11250</td>\n      <td>2.378300</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>2.346700</td>\n    </tr>\n    <tr>\n      <td>11350</td>\n      <td>2.343100</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>2.394000</td>\n    </tr>\n    <tr>\n      <td>11450</td>\n      <td>2.417900</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>2.364800</td>\n    </tr>\n    <tr>\n      <td>11550</td>\n      <td>2.363600</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>2.379900</td>\n    </tr>\n    <tr>\n      <td>11650</td>\n      <td>2.376100</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>2.432200</td>\n    </tr>\n    <tr>\n      <td>11750</td>\n      <td>2.365000</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>2.421600</td>\n    </tr>\n    <tr>\n      <td>11850</td>\n      <td>2.325900</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>2.384500</td>\n    </tr>\n    <tr>\n      <td>11950</td>\n      <td>2.370300</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>2.339500</td>\n    </tr>\n    <tr>\n      <td>12050</td>\n      <td>2.363400</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>2.411500</td>\n    </tr>\n    <tr>\n      <td>12150</td>\n      <td>2.383800</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>2.382600</td>\n    </tr>\n    <tr>\n      <td>12250</td>\n      <td>2.342600</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>2.421100</td>\n    </tr>\n    <tr>\n      <td>12350</td>\n      <td>2.390600</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>2.397800</td>\n    </tr>\n    <tr>\n      <td>12450</td>\n      <td>2.374700</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>2.356800</td>\n    </tr>\n    <tr>\n      <td>12550</td>\n      <td>2.373200</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>2.384400</td>\n    </tr>\n    <tr>\n      <td>12650</td>\n      <td>2.339700</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>2.347700</td>\n    </tr>\n    <tr>\n      <td>12750</td>\n      <td>2.340400</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>2.390300</td>\n    </tr>\n    <tr>\n      <td>12850</td>\n      <td>2.390600</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>2.367200</td>\n    </tr>\n    <tr>\n      <td>12950</td>\n      <td>2.353300</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>2.334000</td>\n    </tr>\n    <tr>\n      <td>13050</td>\n      <td>2.385500</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>2.399500</td>\n    </tr>\n    <tr>\n      <td>13150</td>\n      <td>2.348900</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>2.314000</td>\n    </tr>\n    <tr>\n      <td>13250</td>\n      <td>2.378300</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>2.343100</td>\n    </tr>\n    <tr>\n      <td>13350</td>\n      <td>2.365000</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>2.332000</td>\n    </tr>\n    <tr>\n      <td>13450</td>\n      <td>2.421800</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>2.362700</td>\n    </tr>\n    <tr>\n      <td>13550</td>\n      <td>2.347700</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>2.372800</td>\n    </tr>\n    <tr>\n      <td>13650</td>\n      <td>2.356900</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>2.334700</td>\n    </tr>\n    <tr>\n      <td>13750</td>\n      <td>2.397000</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>2.320600</td>\n    </tr>\n    <tr>\n      <td>13850</td>\n      <td>2.337000</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>2.377700</td>\n    </tr>\n    <tr>\n      <td>13950</td>\n      <td>2.396100</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>2.401000</td>\n    </tr>\n    <tr>\n      <td>14050</td>\n      <td>2.365500</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>2.435900</td>\n    </tr>\n    <tr>\n      <td>14150</td>\n      <td>2.357800</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>2.399900</td>\n    </tr>\n    <tr>\n      <td>14250</td>\n      <td>2.400600</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>2.391100</td>\n    </tr>\n    <tr>\n      <td>14350</td>\n      <td>2.375900</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>2.333800</td>\n    </tr>\n    <tr>\n      <td>14450</td>\n      <td>2.369700</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>2.354600</td>\n    </tr>\n    <tr>\n      <td>14550</td>\n      <td>2.354000</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>2.424900</td>\n    </tr>\n    <tr>\n      <td>14650</td>\n      <td>2.392300</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>2.377400</td>\n    </tr>\n    <tr>\n      <td>14750</td>\n      <td>2.388200</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>2.377100</td>\n    </tr>\n    <tr>\n      <td>14850</td>\n      <td>2.399400</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>2.412900</td>\n    </tr>\n    <tr>\n      <td>14950</td>\n      <td>2.316700</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>2.370400</td>\n    </tr>\n    <tr>\n      <td>15050</td>\n      <td>2.367900</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>2.351700</td>\n    </tr>\n    <tr>\n      <td>15150</td>\n      <td>2.362100</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>2.409100</td>\n    </tr>\n    <tr>\n      <td>15250</td>\n      <td>2.423700</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>2.369500</td>\n    </tr>\n    <tr>\n      <td>15350</td>\n      <td>2.356600</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>2.373500</td>\n    </tr>\n    <tr>\n      <td>15450</td>\n      <td>2.381900</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>2.377800</td>\n    </tr>\n    <tr>\n      <td>15550</td>\n      <td>2.359100</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>2.441700</td>\n    </tr>\n    <tr>\n      <td>15650</td>\n      <td>2.397100</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>2.323300</td>\n    </tr>\n    <tr>\n      <td>15750</td>\n      <td>2.361200</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>2.332700</td>\n    </tr>\n    <tr>\n      <td>15850</td>\n      <td>2.378800</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>2.397800</td>\n    </tr>\n    <tr>\n      <td>15950</td>\n      <td>2.396300</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>2.376000</td>\n    </tr>\n    <tr>\n      <td>16050</td>\n      <td>2.355600</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>2.413300</td>\n    </tr>\n    <tr>\n      <td>16150</td>\n      <td>2.315100</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>2.380500</td>\n    </tr>\n    <tr>\n      <td>16250</td>\n      <td>2.401000</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>2.381600</td>\n    </tr>\n    <tr>\n      <td>16350</td>\n      <td>2.407200</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>2.408900</td>\n    </tr>\n    <tr>\n      <td>16450</td>\n      <td>2.371000</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>2.359800</td>\n    </tr>\n    <tr>\n      <td>16550</td>\n      <td>2.356700</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>2.385600</td>\n    </tr>\n    <tr>\n      <td>16650</td>\n      <td>2.343600</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>2.353200</td>\n    </tr>\n    <tr>\n      <td>16750</td>\n      <td>2.324200</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>2.397000</td>\n    </tr>\n    <tr>\n      <td>16850</td>\n      <td>2.375300</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>2.375200</td>\n    </tr>\n    <tr>\n      <td>16950</td>\n      <td>2.360300</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>2.384600</td>\n    </tr>\n    <tr>\n      <td>17050</td>\n      <td>2.386300</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>2.397600</td>\n    </tr>\n    <tr>\n      <td>17150</td>\n      <td>2.335700</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>2.364200</td>\n    </tr>\n    <tr>\n      <td>17250</td>\n      <td>2.355600</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>2.379400</td>\n    </tr>\n    <tr>\n      <td>17350</td>\n      <td>2.361900</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>2.377200</td>\n    </tr>\n    <tr>\n      <td>17450</td>\n      <td>2.421300</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>2.393800</td>\n    </tr>\n    <tr>\n      <td>17550</td>\n      <td>2.364400</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>2.388700</td>\n    </tr>\n    <tr>\n      <td>17650</td>\n      <td>2.391200</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>2.376400</td>\n    </tr>\n    <tr>\n      <td>17750</td>\n      <td>2.427600</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>2.381900</td>\n    </tr>\n    <tr>\n      <td>17850</td>\n      <td>2.354800</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>2.370500</td>\n    </tr>\n    <tr>\n      <td>17950</td>\n      <td>2.402400</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>2.372100</td>\n    </tr>\n    <tr>\n      <td>18050</td>\n      <td>2.314400</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>2.371400</td>\n    </tr>\n    <tr>\n      <td>18150</td>\n      <td>2.402500</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>2.348100</td>\n    </tr>\n    <tr>\n      <td>18250</td>\n      <td>2.400500</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>2.401600</td>\n    </tr>\n    <tr>\n      <td>18350</td>\n      <td>2.387900</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>2.383000</td>\n    </tr>\n    <tr>\n      <td>18450</td>\n      <td>2.340800</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>2.348100</td>\n    </tr>\n    <tr>\n      <td>18550</td>\n      <td>2.357500</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>2.357200</td>\n    </tr>\n    <tr>\n      <td>18650</td>\n      <td>2.348300</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>2.383600</td>\n    </tr>\n    <tr>\n      <td>18750</td>\n      <td>2.400200</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>2.327900</td>\n    </tr>\n    <tr>\n      <td>18850</td>\n      <td>2.408900</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>2.335100</td>\n    </tr>\n    <tr>\n      <td>18950</td>\n      <td>2.306700</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>2.379600</td>\n    </tr>\n    <tr>\n      <td>19050</td>\n      <td>2.368800</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>2.396700</td>\n    </tr>\n    <tr>\n      <td>19150</td>\n      <td>2.408200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\nThe model 'PeftModel' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n","output_type":"stream"},{"name":"stdout","text":"ğŸ’™ EmpathyBot ready! Try:\nYou: I feel overwhelmed with work.\nBot: I feel overwhelmed with work.  I feel overwhelmed with work.  It's not like I'm a scientist.  That's not fun.  I'm just waiting for the results.  So I don't want to take the time off.  I'm just waiting for results.  So I don't want \n\nYou: My cat is sick and Iâ€™m scared.\nBot: I am hoping it doesn't get too bad. I can't wait for it to get worse.   I am so scared.  I can't wait for it to get worse.    I am so scared.       I can't wait for it to get worse. \n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# -----------------------------------------------------------\n# Cell: CLI chat inside the same Kaggle notebook\n# -----------------------------------------------------------\nimport sys, select, os, torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\nMODEL_ID   = \"distilgpt2\"\nADAPTER    = \"/kaggle/working/empathetic_bot\"\nMAX_TOKENS = 64\nTEMP       = 0.7\n\nprint(\"ğŸ”§ Loading tokenizer â€¦\")\ntokenizer = AutoTokenizer.from_pretrained(ADAPTER)\ntokenizer.pad_token = tokenizer.eos_token\n\nprint(\"ğŸ”§ Loading base model â€¦\")\nbase = AutoModelForCausalLM.from_pretrained(\n    MODEL_ID,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nprint(\"ğŸ”§ Merging LoRA adapter â€¦\")\nmodel = PeftModel.from_pretrained(base, ADAPTER)\nmodel.eval()\n\ndef generate(prompt_text: str) -> str:\n    prompt = f\"<|prompt|>{prompt_text}<|endoftext|><|response|>\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        tokens = model.generate(\n            **inputs,\n            max_new_tokens=MAX_TOKENS,\n            temperature=TEMP,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n            eos_token_id=tokenizer.eos_token_id\n        )\n    text = tokenizer.decode(tokens[0], skip_special_tokens=True)\n    return text.split(\"<|response|>\")[-1].strip()\n\nprint(\"\\nğŸ’™ EmpathyBot CLI ready!  Type /quit to exit.\\n\")\ntry:\n    while True:\n        user = input(\"> \").strip()\n        if not user or user == \"/quit\":\n            print(\"ğŸ‘‹ Good-bye!\")\n            break\n        print(\"Bot:\", generate(user), \"\\n\")\nexcept KeyboardInterrupt:\n    print(\"\\nğŸ‘‹ Good-bye!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-13T13:15:39.921666Z","iopub.execute_input":"2025-08-13T13:15:39.922575Z","iopub.status.idle":"2025-08-13T13:16:22.059612Z","shell.execute_reply.started":"2025-08-13T13:15:39.922540Z","shell.execute_reply":"2025-08-13T13:16:22.058969Z"}},"outputs":[{"name":"stdout","text":"ğŸ”§ Loading tokenizer â€¦\nğŸ”§ Loading base model â€¦\nğŸ”§ Merging LoRA adapter â€¦\n\nğŸ’™ EmpathyBot CLI ready!  Type /quit to exit.\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">  My cat is sick and Iâ€™m scared.\n"},{"name":"stdout","text":"Bot: I hope you get help.  I know you will have a good time.   I hope you can come back at home with a little help. \n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">  I feel overwhelmed with work\n"},{"name":"stdout","text":"Bot: I feel overwhelmed with work  I feel overwhelmed with work  i think  and i feel good about it \n\n\nğŸ‘‹ Good-bye!\n","output_type":"stream"}],"execution_count":3}]}